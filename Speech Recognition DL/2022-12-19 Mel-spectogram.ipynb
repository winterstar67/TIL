{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c5fadc-e3fd-4687-8eab-7c8b31896d9f",
   "metadata": {},
   "source": [
    "# 12월 19일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64fec0-857f-409e-ac75-77fa218ee4d3",
   "metadata": {},
   "source": [
    "### 이번 공부의 목적은 Mel-spectogram의 공부이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b0d07-40d1-443e-acaf-1058d521176d",
   "metadata": {},
   "source": [
    "#### Motivation:\n",
    "Deep learning을 음성 인식이나 음성 합성과 같은 부분에 적용하고 싶어서 공부를 하게 되었다.\n",
    "\n",
    "#### 이 글은 누군가에게 내용을 알려주려고 하는 것이 아닌, 필자가 공부한 내용을 정리하는 용도로 작성하는 것이다.\n",
    "\n",
    "필자가 잘못 이해하고 잘못 작성한 부분도 있을 수 있으니, 공부용으로는 적절치 못할수도 있다.\n",
    "\n",
    "#### 학습 내용의 출처: https://www.youtube.com/watch?v=RxbkEjV7c0o&list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876366c-2aa4-4173-8d20-7a2aa9d4aeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f4351bd-a4df-4c0a-8746-c9298677a297",
   "metadata": {},
   "source": [
    "`-` Auto tagging task는 주로 다음 3가지로 이루어진다.\n",
    "- Audio Representation\n",
    "- Feature Extraction\n",
    "- Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd0ca8-118b-4d88-a97a-58a614008fbf",
   "metadata": {},
   "source": [
    "어떤 label이 있는 task를 받았을 때, label별로 Spectogram을 연산해서 mean value를 취해서 시각화 해보면, label마다 특징들이 속해져있는지에 대한 가설을 가지고 시작할 수 있다 하심.\n",
    "\n",
    "드럼소리를 spectogram을 찍어보고, 이걸 1D convolution을 하면 잘 잡아낸다거나 하는 아이디어를 떠올려볼 수 있음.\n",
    "- 2D CNN을 사용하면 특징을 워낙 잘잡아내서 보통은 Spectogram에 2D CNN 사용한 것을 baseline으로 사용한다 하심.\n",
    "- CNN으로 local한 feature 뽑고 sequential한 음성을 RNN으로 처리하는 것도 생각해볼 수 있다 하심."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70aa7b0-40b5-410d-a7e4-7de913dc19a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd4a8ed1-dd75-4fec-a6de-a22a1acca85a",
   "metadata": {},
   "source": [
    "정현파가 합쳐진 것이 complex wave\n",
    "- 정현파는 복소주기 함수로 나타난다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e5d17-be44-4db8-a081-3eedeaa603b3",
   "metadata": {},
   "source": [
    "푸리에 변환을 하면 실수부와 허수부가 나온다.  \n",
    "실수부, 허수부를 갖는 복소수를 절대값 취하면 실수만 남는데, 이걸 보통 Spectrum magnitude라고 부른다.\n",
    "- 여기서 허수부를 빼고 보면 Phase spectrum이라고도 부른다. 이게 뭐냐면, 소리에는 세기와 주파수, 위상(phase)가 있는데, 이 주파수안에 강도까지 포함되서 나오는게 spectrum magnitude이고(각 주파수가 어느정도의 강도를 가지고 있다는 것). 그 다음에, phase는 어떤 wave form의 위상으로 표현됨.\n",
    "\n",
    "정리하면, 어떤 소리를 푸리에 변환을 하면 실수부와 허수부를 갖는 걸로 return이 되는데, 보통 실수가 주파수대의 magnitude를 뜻하고 허수부가 phase를 뜻한다.\n",
    "- 완전 같은건 아닌데, 이렇게 생각해도 된다하셨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e3f90-887e-468d-bebb-ed59d45780f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d58cff9-3a17-4406-ba80-21a723094868",
   "metadata": {},
   "source": [
    "DFS는 푸리에 변환을 discrete한 영역에서 하기 위해 나온 것.  \n",
    "수집한 데이터(시그널)의 0번째 인덱스 찍었을 때 나왔던 연속적인 데이터, 이 시그널 데이터는 여러 영역대의 주파수와 진폭이 다른 n개의 주기 함수의 합으로 표현이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506aa2f1-a46e-46ac-898d-4299d7984d45",
   "metadata": {},
   "source": [
    "어떤 스펙토그램을 보더라도 주파수와 시간으로 표현된다.   \n",
    "어떤 소리가 시간에 진행되는데, 이전의 wave form은 amplitude만 시각화가 되었다면, 지금은 어떤 frequency 영역대가 강한지 시각화 시켜주는 것이 스펙토그램이고, 이 값을 얻어내는게 보통 푸리에 변환으로 얻어지게 된다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a7d65-6de9-4111-9990-89a5487d9079",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc70cfc-652a-4ec5-83be-9048dc9f7549",
   "metadata": {},
   "source": [
    "`-` `여긴 확실히 파악한지 확신이 안섬`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbd288-1e66-476c-9b61-3f9091f7a46b",
   "metadata": {},
   "source": [
    "STFT는 짧은 시간마다 푸리에 변환을 적용하는 것.  \n",
    "librosa.core.stft(audio data, n_fft, hop_length, window_length)를 통해서 STFT를 적용할 수 있다.\n",
    "\n",
    "이 라이브러리의 함수를 적용하면 실수부와 허수부가 나오는데, 보통 허수부는 소리를 인지하는데 큰 정보가 없다 생각하여 날린다고 한다.  \n",
    "그런데 Phase가 안맞으면 음성합성에서 사람목소리로 안들릴 수도 있어서 Phase 정보 날릴때는 보통 절대값에 제곱 취하는 경향이 많다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e82b1d-aa6a-491b-a392-fabf87e1488c",
   "metadata": {},
   "source": [
    "- 이 부분에 관한 설명은 https://judy-son.tistory.com/6 여기 자세히 있다.\n",
    "- https://newsight.tistory.com/294 : 음성인식 기초 이해하기\n",
    "- https://xangmin.tistory.com/61: Mel spectogram\n",
    "- https://dacon.io/en/codeshare/5153\n",
    "\n",
    "보면 mel spectogram은, 일단 spectogram 자체가 시간에 따른 주파수를 기록한 plot인데, 여기서 이 주파수의 단위를 mel unit으로 변경한 것이다. 이 때 mel scale은 발견한 사람의 음을 인지하는 기준을 반영한 scale 변환함수라 한다. exponential해서 이를 log를 취해준거라 함. Mel-Spectrogram의 경우는 주파수끼리 Correlate하기 때문에 도메인이 한정적인 문제에서 더 좋은 성능을 보인다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7380e4b-b21e-45f5-8ef0-647ed47fc912",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99dc3ea8-8838-4341-bcda-adb3180639b8",
   "metadata": {},
   "source": [
    "여기서 hop_length를 줄이면 time domain이 늘어난다고 한다. 이 때 Mel spectogram이 Neural 아키텍처에 들어간다 할 때, 이전에 쓰던 아키텍처가 224*224 이미지에 최적화된 아키텍처를 쓰고 있는데, time length가 너무 길다 싶으면 hop size를 늘리면서 time length를 줄여보는 것도 방법이고, 이건 hyper parameter 라 돌려보면서 쓰는데, 강사분이 권장하는건 1024, 512, 1024 정도로 하면 baseline으로 쓸만하다 하심.  \n",
    "여기서 Convolution layer 한번 진행되면 그 다음 shape이 어떻게 될지 계산해야되니까 input shape이 중요한 요소가 되는데, 우리가 이런 오디오를 자른다는 개념을 알면 좋음. FFT를 돌릴 때 1024개가 하나의 프레임이다 라고 지정한 것, 그러면 이게 0.n초의 frequency rate가 1초에 1만 6천개다. 그러면 1024면 1024/16000하면 몇초정도가 들어가는지 대략 알 수 있게됨. (https://youtu.be/FjYNM3YGFB4?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=1509)\n",
    "\n",
    "그러면 사람들이 소리를 오디오 input을 인지할 때, 그 정도 소리로 인지하는가?는 task마다 다름.  \n",
    "일반적인 sound다. 외부소리다. 라고 하면, 빨리빨리 프로세싱을 거치는데, 음악이면 음악의 코드진행을 보면서 느껴지는게 달라져서 length를 키워본다던가. 이런식으로 시도를 해볼 수 있음. 그래서 frequency를 어느정도 사이즈로 넣고, 프레임이 어떻게 정의되고 이런걸 알면 좋음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d464c13-0830-4dee-ac23-bc90d1b770e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bea3f42-1703-4477-b477-7346b325f0de",
   "metadata": {},
   "source": [
    "시간마다 주파수가 달라짐 (wave가 항상 일정한게 아님). 그래서 이걸 Spectogram으로 알 수 있음.  \n",
    "여기서, 보통 저주파수가 고주파수보다 정보가 더 풍부함, 보통 이쪽 음성인식이나 음악쪽 공부하다 보면 fundamental frequency(F0)라는 개념이 있는데, 이 F0를 제거했을 때 소리에 대한 인식률이 확실히 떨어진다함. 어떤 소리가 형성됬을 때, 가장 낮은 음에 해당하는 것들이 fundamental을 이루게 되고, 그 fundamental에 대해서 굉장히 큰 정보가 있다고 생각을 함.\n",
    "\n",
    "그런데, 인간의 청각경로 상에서도 보면 저주파에 대한 정보를 더 크게 인지하는 경향이 있다 하심. (https://youtu.be/FjYNM3YGFB4?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=1717)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2119402-9667-4f30-a97f-ce9d20762c7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c91bb92b-b512-4a92-9549-ed2686ff9f2b",
   "metadata": {},
   "source": [
    "사람의 소리 인지와 관련된 개념이 Mel-Filter bank이다.\n",
    "- 사람은 인접한 주파수를 크게 구별하지 못한다.\n",
    "- low frequency에서 더 풍부한 정보를 습득하게 된다.\n",
    "\n",
    "그렇다면, 우리가 이런 스펙토그램이 linear한 스케일로 input으로 들어가는게 아니라, 조금 더 저주파수 영역의 정보를 더 반영할 수 있는 스케일로 들어가야하지 않을까 라는 부분에서 나온게, frequency의 filter bank 개념이다.\n",
    "\n",
    "사람들이 소리를 인지하는거랑 최대한 유사하게 input으로 넣어야지 Neural net도 성능이 좋고, 어떤 TTS나 STT 모형들도 좋지 않을까. 라고 생각을 했고, 실제로도 우리가 Mel-frequency를 썼을 때, 좀더 performance가 올라가는 현상이 나온다.  \n",
    "그래서 음악도 그렇고, 오디오도 그렇고, 보통 input으로 log나 mel 스케일로 filter bank를 적용해서 scaling을 한번 한 것을 넣게 됨.  \n",
    "\n",
    "mu-alw encoding을 잘 안하는데 (강사분 생각은) 그 이유중 하나가, 사람 귀가 소리에 되게 log적으로 반응하는데, 예를들어 조그만한 소리는 잘 잡는데, 클럽가서 base가 펑펑 울리는 소리에 장시간 노출되면 소리의 차이를 크게 인지하지 못함(귀가 멍해지기도 하고).\n",
    "그래서 이러한 특성들을 잘 분별하기 위해, 우리가 mu-law라 해서 어떤 스케일을 log 스케일로 바꿔서 인지하면 더 좋지 않을까 해서 만든건데, 이는 amplitude에 대한 전처리임.  \n",
    "\n",
    "일단 이러한 전처리 방법론이 있고, 전처리 방법론이 계속해서 나오고 있음. 큰 흐름으로 보면 사람들이 소리를 어떻게 인식하느냐에 대한 것을 최대한 따라가고 있다는 것. mel frequency도 mel bin이라는 fequency영역대의 bin을 특정 weight를 곱해주면서 보곘다 라는 것. (https://youtu.be/FjYNM3YGFB4?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=2004)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a84914-8aa2-40b3-aa36-36d0cd87e41f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb06e8fc-2fb1-4f23-8664-f1ccd8396a89",
   "metadata": {},
   "source": [
    "raw한 input을 그대로 쓰기도 함. resnet의 경우는 wave signal을 그대로 쓰기도 함.  \n",
    "그런데, end to end로 가면서 mel filter bank나 sepctogram에서 우리가 보고싶어하는 주파수 영역대나, 이런 것 까지 end to end로 학습할 수 있지 않을까, 하는 생각도 학계쪽에서 있는 것 같지만 아직까진 spectogram 레벨로 input이 많이 들어가고, 점점 wave form으로 내려가려는 추세이긴 한 것 같다 하심.  \n",
    "최대한 feature engineering이 적을수록 contribution이 있다는게 학계의 움직임인 것 같다 하심."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6ec3d-c654-48fa-a1c3-7ae14fe2d965",
   "metadata": {},
   "source": [
    "`-` 여기까지 보고, 일단은 우린 mel spectogram을 사용할 것이고, 이것은 librosa에 구현이 되어있음.\n",
    "- mel 함수: librosa.filters.mel() 함수로 구현되어있다. 보통 n_mels 인자 값은 40, 92, 128 이렇게 본다. (음성쪽은 보통 40, 노래쪽은 92, 128 이렇게 자주 보셨다고 한다.)\n",
    "    - https://youtu.be/FjYNM3YGFB4?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=2095"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb51b36-f971-440f-aea9-d1c668e968be",
   "metadata": {},
   "source": [
    "강사님께서 mel bin 값 설정하는 방법: https://youtu.be/FjYNM3YGFB4?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=2259"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5985a3-84c9-4bef-beac-a36e8416c1e2",
   "metadata": {},
   "source": [
    "`-` mel bin은 어떤 frequency를 커버하는 영역 결정하는 것 같음. bin을 잘게 쪼개면 high frqeuncy를 커버할 수 있는듯. ( https://youtu.be/FjYNM3YGFB4?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=2259 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad32912-68ad-4fe0-8bc1-ff1c70b8ad12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aaa46ad-675c-4266-891d-b8638a385106",
   "metadata": {},
   "source": [
    "wave form과 spectogram의 차이:\n",
    "- wave form은 amplitude값만 찍히는 값을 시간축에 따라 보겠다.\n",
    "- spectogram은 푸리에 변환을 거쳐서, 어떤 frequency 영역대에서 활성화 되었는지를 시간축에 따라서 보곘다.\n",
    "- wave form은 amplitude만, spectogram은 frequecny와 amplitude가 같이있다. 정도로 알아두면 된다 하심.\n",
    "- https://youtu.be/FjYNM3YGFB4?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=2370\n",
    "\n",
    "Phase도 있긴한데, 잘 안쓰이고 음성합성쪽에서 쓰인다고 하심. 그런데 classification이나 auto tagging쪽에선 그렇게 중요한 것 같진 않다하심.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff049f5-0da3-46a2-860a-bcec4fdd49de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70cf1845-b60d-40b4-97ce-c60830eae142",
   "metadata": {},
   "source": [
    "amplitude와 power 관련 질문\n",
    "- https://youtu.be/kiTHOCmWPsg?list=PL9mhQYIlKEhem5_wrQqDtNqNcaDyFrYGN&t=12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179dcf6-33f9-4dd8-a01d-be6d7f68a527",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f17500d6-a663-4639-bb36-b7547c080179",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66fba0c7-2fd3-4477-8d03-d961fa5b2a88",
   "metadata": {},
   "source": [
    "**이번 내용은 추후에 추가적으로 정리**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
