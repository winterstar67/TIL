{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511e6f4b-ac61-479d-aa8d-40721b05c928",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DeepDTA(논문을 참고한 Data 처리부터 모델 구현)\n",
    "> 2022-10-18\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: blog_owner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb51ba0-1bf0-4c39-8840-b7bbef150194",
   "metadata": {},
   "source": [
    "`-` 이전의 블로그에서 가져왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86acac-15fa-4450-915c-a0a363aff942",
   "metadata": {},
   "source": [
    "`-` 이번 모델 구현은 DeepDTA를 기반으로 하긴 하였지만, 논문 참고만 하고 정답같은 것 없이 데이터 처리, 모델 등을 구현한 것이라 DeepDTA 구현 모델이라고 하긴 힘들 것 같다.\n",
    "\n",
    "`-` 모델을 만드는데 사용한 라이브러리는 pytorch이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c93332-306d-483d-972d-28b8542e9135",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab43630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neogreen/anaconda3/envs/chihyeon/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a26777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ab72c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('use GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('use CPU')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1820c-1321-4385-b018-0ed4cdb5888a",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d440d83",
   "metadata": {},
   "source": [
    "## Drug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98942ae-5990-40d6-b17e-9705d302e6d4",
   "metadata": {},
   "source": [
    "`-` 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1195f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"drug.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3307845-ad81-4ce8-9f0e-137c9871d6fc",
   "metadata": {},
   "source": [
    "`-` SMILES만 남도록 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6b676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = list(str(lines).split())\n",
    "drugs = []\n",
    "for i in range(len(drug)//2):\n",
    "    drugs.append(drug[2*i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dbfc134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "__drugs = list(map(lambda s: s[1:-2], drugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d1c41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "__drugs[-1] = __drugs[-1][:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174b83a-bfaf-4d35-a697-83d0199063b5",
   "metadata": {},
   "source": [
    "`-` drug를 데이터 프레임 형태로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b150e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = __drugs\n",
    "_drug = pd.DataFrame({\"drugs\":drugs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981e36a",
   "metadata": {},
   "source": [
    "## Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52771651",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proteins.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da98f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = list(str(lines).split())\n",
    "proteins = []\n",
    "for i in range(len(protein)//2):\n",
    "    proteins.append(protein[2*i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4a9b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "__proteins = list(map(lambda s: s[1:-2], proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94894df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "__proteins[-1] = __proteins[-1][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5b22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = __proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a564d7-7fe1-458e-9afd-f4d89b3bb859",
   "metadata": {},
   "source": [
    "`-` sequence안의 각 amino acid character를 모아서 vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cca45801",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_proteins = ''.join(map(str, proteins)) + \"@\" # \"@\" for padding or End\n",
    "proteins_vocab = set(set_proteins)\n",
    "proteins_vocab_size = len(set(set_proteins))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a703b9-2b27-44aa-9607-aea3ddfe240d",
   "metadata": {},
   "source": [
    "`-` protein 데이터를 Dataframe 형태로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03244493",
   "metadata": {},
   "outputs": [],
   "source": [
    "_protein =  pd.DataFrame({\"proteins\":proteins})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0694bc-8583-417d-9472-415b4b9eb640",
   "metadata": {},
   "source": [
    "## Drug와 Protein 데이터를 하나의 Data frame에 묶음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb3bd6-5dbb-4c88-9815-2ff49012b372",
   "metadata": {},
   "source": [
    "`-` 최종적으로 사용할 데이터는 Drug와 Protein간의 affinity이다.  \n",
    "데이터는 Drug 1과 Protein 1의 affinity, Drug 1과 Protein 2의 affinity, Drug 1과 Protein 3의 affinity, ... , Drug m과 Protein n의 affinity 와같은 형태로 각 drug마다 모든 protein에 대한 affinity 형태로 데이터가 주어져있다.\n",
    "\n",
    "따라서 둘을 합칠 때는 Drug 1, Drug 2, ... Drug m 각각 에 대해서 Broadcasting을 하는 방식으로 protein과 데이터를 합쳐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f312ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _drug.merge(_protein, how='cross')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9f202-1989-4878-b7c4-d53b9b3b4f0d",
   "metadata": {},
   "source": [
    "- 위와같은 broadcasting을 하면서 병합하는 방법이 merge안의 인자에 how='cross'를 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac6b5d-a968-4fec-a628-0550d612effd",
   "metadata": {},
   "source": [
    "`-` 이제 각 drug와 protein의 affinity 값들을 불러와서 위에서 만든 drug-protein pair마다 해당 값들을 넣어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23ea6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"drug_protein.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f44f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(map(lambda x: x[:-1], lines))\n",
    "lines = list(map(lambda x: x.split(), lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b65fd2",
   "metadata": {},
   "source": [
    "`-` 이걸 이제, 각각의 요소를 str에서 float 타입으로 바꿔야하고, 하나 벡터로 flatten해서 펼쳐서 그대로 dataframe에 affinity를 ㅜ이한 column하나를 추가해서 붙여야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c12ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lines)):\n",
    "    lines[i] = list(map(lambda x: float(x), lines[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbeaf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_protein_affinity = torch.tensor(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90924382",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_protein = drug_protein_affinity.reshape([-1]) # 이게 잘 flatten 됬는지 확인하려면 drug_protein_affinity[0]이랑 drug_protein 의 첫번째 drug의 모든 protein에 대한 affinitiy인 442개의 첫번째 요소들이 같은지 보면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e393b62-8632-4b51-8e2f-8964a1119065",
   "metadata": {},
   "source": [
    "`-` 논문에서는 affinity를 그냥 사용하지 않고 10^9로 나누고 log를 적용한 값을 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e94b48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity = -torch.log10(drug_protein/(10**9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b1429",
   "metadata": {},
   "source": [
    "- 논문에서 affinity 그대로 안쓰고 계산과정 한번 거침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c23e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"affinity\"] = affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a2fd1-416b-4889-8c85-5361ab79234d",
   "metadata": {},
   "source": [
    "`-` 중복되는 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0267fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f446868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data[[\"drugs\",\"proteins\"]].drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24298e3a-43ba-4565-9d43-481dd68961f8",
   "metadata": {},
   "source": [
    "`-` 이전에 merge를 할 때 broadcast방식으로 해서 데이터의 순서가 Drug 1과 Protein 1의 affinity, Drug 1과 Protein 2의 affinity, Drug 1과 Protein 3의 affinity 이런식으로 되었다.  \n",
    "이 상태로 training[0:0.5], test[0.5:1] 로 나눠버리고 학습을 진행한다면 0.5 이후의 Drug Smiles는 training동안에 못본 데이터여서 성능이 잘 안나올 것이다. \n",
    "\n",
    "따라서 아래와 같은 코드로 shuffle을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5eda394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drugs</th>\n",
       "      <th>proteins</th>\n",
       "      <th>affinity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10519</th>\n",
       "      <td>C1CC(=NO)C2=C1C=C(C=C2)C3=CN(N=C3C4=CC=NC=C4)CCO</td>\n",
       "      <td>MPARIGYYEIDRTIGKGNFAVVKRATHLVTKAKVAIKIIDKTQLDE...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7103</th>\n",
       "      <td>CC(C1=C(C=CC(=C1Cl)F)Cl)OC2=C(N=CC(=C2)C3=CN(N...</td>\n",
       "      <td>MEGAAAPVAGDRPDLGLGAPGSPREAVAGATAALEPRKPHGVKRHH...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16942</th>\n",
       "      <td>CCOC1=C(C=C2C(=C1)N=CC(=C2NC3=CC(=C(C=C3)OCC4=...</td>\n",
       "      <td>MCTVVDPRIVRRYLLRRQLGQGAYGIVWKAVDRRTGEVVAIKKIFD...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7334</th>\n",
       "      <td>CC(C1=C(C=CC(=C1Cl)F)Cl)OC2=C(N=CC(=C2)C3=CN(N...</td>\n",
       "      <td>MAHSPVAVQVPGMQNNIADPEELFTKLERIGKGSFGEVFKGIDNRT...</td>\n",
       "      <td>5.552842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21242</th>\n",
       "      <td>CN(C)CC1CCN2C=C(C3=CC=CC=C32)C4=C(C5=CN(CCO1)C...</td>\n",
       "      <td>MSDVTIVKEGWVQKRGEYIKNWRPRYFLLKTDGSFIGYKEKPQDVD...</td>\n",
       "      <td>5.853872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19552</th>\n",
       "      <td>CC(C)N1C2=C(C(=C3C=C4C=C(C=CC4=N3)O)N1)C(=NC=N2)N</td>\n",
       "      <td>MACLHETRTPSPSFGGFVSTLSEASMRKLDPDTSDCTPEKDLTPTH...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C...</td>\n",
       "      <td>MRHSKRTHCPDWDSRESWGHESYRGSHKRKRRSHSSTQENRHCKPH...</td>\n",
       "      <td>5.677781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13284</th>\n",
       "      <td>CS(=O)(=O)CCNCC1=CC=C(O1)C2=CC3=C(C=C2)N=CN=C3...</td>\n",
       "      <td>MSDVAIVKEGWLHKRGEYIKTWRPRYFLLKNDGTFIGYKERPQDVD...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CN(C)CC=CC(=O)NC1=C(C=C2C(=C1)C(=NC=N2)NC3=CC(...</td>\n",
       "      <td>MKPATGLWVWVSLLVAAGTVQPSDSQSVCAGTENKLSSLSDLEQQY...</td>\n",
       "      <td>8.200660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6478</th>\n",
       "      <td>C=CC(=O)NC1=C(C=C2C(=C1)C(=NC=N2)NC3=CC(=C(C=C...</td>\n",
       "      <td>MSDGLDNEEKPPAPPLRMNSNNRDSSALNHSSKPLPMAPEEKNKKA...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   drugs  \\\n",
       "10519   C1CC(=NO)C2=C1C=C(C=C2)C3=CN(N=C3C4=CC=NC=C4)CCO   \n",
       "7103   CC(C1=C(C=CC(=C1Cl)F)Cl)OC2=C(N=CC(=C2)C3=CN(N...   \n",
       "16942  CCOC1=C(C=C2C(=C1)N=CC(=C2NC3=CC(=C(C=C3)OCC4=...   \n",
       "7334   CC(C1=C(C=CC(=C1Cl)F)Cl)OC2=C(N=CC(=C2)C3=CN(N...   \n",
       "21242  CN(C)CC1CCN2C=C(C3=CC=CC=C32)C4=C(C5=CN(CCO1)C...   \n",
       "...                                                  ...   \n",
       "19552  CC(C)N1C2=C(C(=C3C=C4C=C(C=CC4=N3)O)N1)C(=NC=N2)N   \n",
       "12017  CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C...   \n",
       "13284  CS(=O)(=O)CCNCC1=CC=C(O1)C2=CC3=C(C=C2)N=CN=C3...   \n",
       "1908   CN(C)CC=CC(=O)NC1=C(C=C2C(=C1)C(=NC=N2)NC3=CC(...   \n",
       "6478   C=CC(=O)NC1=C(C=C2C(=C1)C(=NC=N2)NC3=CC(=C(C=C...   \n",
       "\n",
       "                                                proteins  affinity  \n",
       "10519  MPARIGYYEIDRTIGKGNFAVVKRATHLVTKAKVAIKIIDKTQLDE...  5.000000  \n",
       "7103   MEGAAAPVAGDRPDLGLGAPGSPREAVAGATAALEPRKPHGVKRHH...  5.000000  \n",
       "16942  MCTVVDPRIVRRYLLRRQLGQGAYGIVWKAVDRRTGEVVAIKKIFD...  5.000000  \n",
       "7334   MAHSPVAVQVPGMQNNIADPEELFTKLERIGKGSFGEVFKGIDNRT...  5.552842  \n",
       "21242  MSDVTIVKEGWVQKRGEYIKNWRPRYFLLKTDGSFIGYKEKPQDVD...  5.853872  \n",
       "...                                                  ...       ...  \n",
       "19552  MACLHETRTPSPSFGGFVSTLSEASMRKLDPDTSDCTPEKDLTPTH...  5.000000  \n",
       "12017  MRHSKRTHCPDWDSRESWGHESYRGSHKRKRRSHSSTQENRHCKPH...  5.677781  \n",
       "13284  MSDVAIVKEGWLHKRGEYIKTWRPRYFLLKNDGTFIGYKERPQDVD...  5.000000  \n",
       "1908   MKPATGLWVWVSLLVAAGTVQPSDSQSVCAGTENKLSSLSDLEQQY...  8.200660  \n",
       "6478   MSDGLDNEEKPPAPPLRMNSNNRDSSALNHSSKPLPMAPEEKNKKA...  5.000000  \n",
       "\n",
       "[25772 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1)\n",
    "data = data.sample(frac=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98df16-d758-49cd-876c-1e826c9ca36f",
   "metadata": {},
   "source": [
    "- 같은 SMILES가 연속으로 나오지 않는 것을 통해 Shuffle이 잘 되었음을 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab6708-a3b7-4ab6-b4c4-3f9ff9335e0d",
   "metadata": {},
   "source": [
    "`-` char를 int로 바꾸는 작업 + max length에 미치지 못하는 sequnece는 @에 대응되는 integer로 추가로 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1ca6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(x, length):\n",
    "    set_x = ''.join(map(str, x)) + \"@\"\n",
    "    \n",
    "#    print(x[0])\n",
    "\n",
    "    set_x = set_x + \"@\" # \"@\" for padding or End\n",
    "    x_vocab = set(set_x)\n",
    "    \n",
    "    x_word_dict = {w: i for i, w in enumerate(x_vocab)}\n",
    "    x_number_dict = {i: w for i, w in enumerate(x_vocab)}\n",
    "    \n",
    "#    print(x_word_dict)\n",
    "    \n",
    "    len_x = list(map(lambda s: len(s), x))\n",
    "    max_len_x = max(len_x)\n",
    "    \n",
    "    \n",
    "    x_mapped = []\n",
    "    for i in x:\n",
    "        one_molecule = []\n",
    "        for j in i:\n",
    "            if(len(one_molecule)<length):\n",
    "                try:\n",
    "                    one_molecule.append(x_word_dict[j])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        while(len(one_molecule)<length):  # 논문에서 85로 제한함\n",
    "            one_molecule.append(x_word_dict[\"@\"])\n",
    "\n",
    "        x_mapped.append(one_molecule)\n",
    "        \n",
    "    \n",
    "\n",
    "    return x_mapped, len(x_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf3a8d-cc57-41af-8b9c-54f1914a70b6",
   "metadata": {},
   "source": [
    "`-` drug와 protein에 대해서 각각 character를 dictionary의 대응되는 integer로 바꾸고 설정한 max 길이만큼 모자라면 @로 패딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca6f2b3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1CC(=NO)C2=C1C=C(C=C2)C3=CN(N=C3C4=CC=NC=C4)CCO\n",
      "{'O': 0, '7': 1, 'P': 2, 'C': 3, 'F': 4, 'l': 5, '6': 6, '8': 7, '(': 8, 'B': 9, '2': 10, 'I': 11, '5': 12, '9': 13, 'S': 14, '=': 15, 'N': 16, '#': 17, '.': 18, ')': 19, '4': 20, '@': 21, 'r': 22, '1': 23, '3': 24}\n",
      "MPARIGYYEIDRTIGKGNFAVVKRATHLVTKAKVAIKIIDKTQLDEENLKKIFREVQIMKMLCHPHIIRLYQVMETERMIYLVTEYASGGEIFDHLVAHGRMAEKEARRKFKQIVTAVYFCHCRNIVHRDLKAENLLLDANLNIKIADFGFSNLFTPGQLLKTWCGSPPYAAPELFEGKEYDGPKVDIWSLGVVLYVLVCGALPFDGSTLQNLRARVLSGKFRIPFFMSTECEHLIRHMLVLDPNKRLSMEQICKHKWMKLGDADPNFDRLIAECQQLKEERQVDPLNEDVLLAMEDMGLDKEQTLQSLRSDAYDHYSAIYSLLCDRHKRHKTLRLGALPSMPRALAFQAPVNIQAEQAGTAMNISVPQVQLINPENQIVEPDGTLNLDSDEGEEPSPEALVRYLSMRRHTVGVADPRTEVMEDLQKLLPGFPGVNPQAPFLQVAPNVNFMHNLLPMQNLQPTGQLEYKEQSLLQPPTLQLLNGMGPLGRRASDGGANIQLHAQQLLKRPRGPSPLVTMTPAVPAVTPVDEESSDGEPDQEAVQSSTYKDSNTLHLPTERFSPVRRFSDGAASIQAFKAHLEKMGNNSSIKQLQQECEQLQKMYGGQIDERTLEKTQQQHMLYQQEQHHQILQQQIQDSICPPQPSPPLQAACENQPALLTHQLQRLRIQPSSPPPNHPNNHLFRQPSNSPPPMSSAMIQPHGAASSSQFQGLPSRSAIFQQQPENCSSPPNVALTCLGMQQPAQSQQVTIQVQEPVDMLSNMPGTAAGSSGRGISISPSAGQMQMQHRTNLMATLSYGHRPLSKQLSADSAEAHSLNVNRFSPANYDQAHLHPHLFSDQSRGSPSSYSPSTGVGFSPTQALKVPPLDQFPTFPPSAHQQPPHYTTSALQQALLSPTPPDYTRHQQVPHILQGLLSPRHSLTGHSDIRLPPTEFAQLIKRQQQQRQQQQQQQQQQEYQELFRHMNQGDAGSLAPSLGGQSMTERQALSYQNADSYHHHTSPQHLLQIRAQECVSQASSPTPPHGYAHQPALMHSESMEEDCSCEGAKDGFQDSKSSSTLTKGCHDSPLLLSTGGPGDPESLLGTVSHAQELGIHPYGHQPTAAFSKNKVPSREPVIGNCMDRSSPGQAVELPDHNGLGYPARPSVHEHHRPRALQRHHTIQNSDDAYVQLDNLPGMSLVAGKALSSARMSDAVLSQSSLMGSQQFQDGENEECGASLGGHEHPDLSDGSQHLNSSCYPSTCITDILLSYKHPEVSFSMEQAGV\n",
      "{'Q': 0, 'H': 1, 'P': 2, 'C': 3, 'F': 4, 'G': 5, 'A': 6, 'I': 7, 'D': 8, 'E': 9, 'S': 10, 'T': 11, 'W': 12, 'M': 13, 'Y': 14, 'N': 15, 'V': 16, 'R': 17, 'K': 18, 'X': 19, 'L': 20, '@': 21}\n"
     ]
    }
   ],
   "source": [
    "_drugs, _drug_vocab = word2int(data[\"drugs\"].values,85)\n",
    "_proteins, _protein_vocab = word2int(data[\"proteins\"].values,1200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7929cb5",
   "metadata": {},
   "source": [
    "- data[\"drugs\"]에서 data[\"drugs\"].values 로 바꿈, 일단 values 안하면 dictionary 길이가 protein의 경우는 49까지 나옴, 즉 뭔가 의도치않은 character들이 여러개 들어간다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98788c35-638d-41b8-9215-a22137836e5f",
   "metadata": {},
   "source": [
    "`-` Training을 위해 affinity를 텐서화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e18a612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_affinity = torch.tensor(data[\"affinity\"].values).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10595565-19e5-4bc1-afb4-2bff6a03bfab",
   "metadata": {},
   "source": [
    "`-` _drug와 _protein은 현재 word2int를 통해 char로 이루어진 sequence에서 int로 이루어진 sequence가 된 꼴이다. 이 int를 pytorch의 tensor로 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d52c7bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_d = torch.tensor(_drugs, dtype=int)\n",
    "model_input_p = torch.tensor(_proteins, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae68466-5613-40e5-833b-661eb37d2e5b",
   "metadata": {},
   "source": [
    "`-` drug, protein, affinity를 dim = 1에 대해서 concat을 하였다. 나중에 training할 때는 drug, protein, affinity의 길이를 기억해서 인덱싱으로 나눌 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1735278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = torch.cat([model_input_d, model_input_p,_affinity], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29670a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0000, 23.0000,  3.0000,  ..., 20.0000, 13.0000,  5.0000],\n",
       "        [ 3.0000,  3.0000,  8.0000,  ..., 21.0000, 21.0000,  5.0000],\n",
       "        [ 3.0000,  3.0000,  0.0000,  ..., 21.0000, 21.0000,  5.0000],\n",
       "        ...,\n",
       "        [ 3.0000, 14.0000,  8.0000,  ..., 21.0000, 21.0000,  5.0000],\n",
       "        [ 3.0000, 16.0000,  8.0000,  ...,  9.0000,  8.0000,  8.2007],\n",
       "        [ 3.0000, 15.0000,  3.0000,  ..., 21.0000, 21.0000,  5.0000]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b0778-a7ea-4f2d-875a-fc1180d6ddac",
   "metadata": {},
   "source": [
    "`-` training DataLoader, test DataLoader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6feb1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(model_input[:round(25772 *0.8)], batch_size=256, shuffle=True , drop_last = True)\n",
    "test_loader = DataLoader(model_input[round(25772 *0.8):], batch_size=256, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea7555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23bceca-45c6-431c-97b7-a25363f39199",
   "metadata": {},
   "source": [
    "`-` 모델 파라메터 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b2ed6a37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 128]) 4736\n",
      "torch.Size([49, 128]) 6272\n",
      "torch.Size([32, 128, 4]) 16384\n",
      "torch.Size([64, 32, 6]) 12288\n",
      "torch.Size([96, 64, 8]) 49152\n",
      "torch.Size([32, 128, 4]) 16384\n",
      "torch.Size([64, 32, 8]) 16384\n",
      "torch.Size([96, 64, 12]) 73728\n",
      "torch.Size([1024, 192]) 196608\n",
      "torch.Size([1024, 1024]) 1048576\n",
      "torch.Size([512, 1024]) 524288\n",
      "torch.Size([1, 512]) 512\n",
      "1965312\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in model.parameters():\n",
    "    if(parameter.dim() >= 2):\n",
    "        # print(parameter.shape, np.prod(parameter.shape))\n",
    "        count += np.prod(parameter.shape)\n",
    "        \n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68947d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b53b987",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d18a25d",
   "metadata": {},
   "source": [
    "`-`  내가 만든 모델에 내가 놓쳐서 넣지 않은 파트(AdaptiveMaxPool1d와 Conv 중간중간에 ReLU) 추가 + 다른 트레이닝 코드로 나온 결과 (내가 만든 트레이닝 코드가 아님)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "981d4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, drug_n_in_channel = 85, protein_n_in_channel = 1200 , n_filters = 32, drug_kernel_size = [8,8,8], protein_kernel_size = [8,8,8]):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.smiles_emb = nn.Embedding(25,128)\n",
    "        self.protein_emb = nn.Embedding(25,128)\n",
    "        \n",
    "        self.smiles_Conv1 = nn.Conv1d(in_channels = 128, out_channels = n_filters, kernel_size = drug_kernel_size[0] )\n",
    "        self.smiles_Conv2 = nn.Conv1d(in_channels = n_filters, out_channels = n_filters*2, kernel_size = drug_kernel_size[1] )\n",
    "        self.smiles_Conv3 = nn.Conv1d(in_channels = n_filters*2, out_channels = n_filters*3, kernel_size = drug_kernel_size[2] )        \n",
    "        self.smiles_MaxPool = nn.MaxPool1d(kernel_size = drug_kernel_size[0], stride = 1)\n",
    "        self.smiles_AdaMaxPool = nn.AdaptiveMaxPool1d(1)   # 원래 내가 만든 model엔 없던 파트, 이거말고 MaxPool 사용했음\n",
    "\n",
    "        \n",
    "        self.protein_Conv1 = nn.Conv1d(in_channels = 128, out_channels = n_filters, kernel_size = protein_kernel_size[0] )\n",
    "        self.protein_Conv2 = nn.Conv1d(in_channels = n_filters, out_channels = n_filters*2, kernel_size = protein_kernel_size[1] )\n",
    "        self.protein_Conv3 = nn.Conv1d(in_channels = n_filters*2, out_channels = n_filters*3, kernel_size = protein_kernel_size[2] )\n",
    "        self.protein_MaxPool = nn.MaxPool1d(kernel_size = protein_kernel_size[0], stride = 1 )\n",
    "        self.protein_AdaMaxPool = nn.AdaptiveMaxPool1d(1)  # 원래 내가 만든 model엔 없던 파트, 이거말고 MaxPool 사용함\n",
    "        \n",
    "        self.linear1 = nn.Linear(192  ,1024, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(1024, 1024, bias=True)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.linear3 = nn.Linear(1024, 512, bias=True)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "        \n",
    "        self.linear_final = nn.Linear(512, 1, bias=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, drug, protein):\n",
    "        \n",
    "       # print(drug.is_cuda)\n",
    "       # print(protein.is_cuda)\n",
    "        \n",
    "        X = self.smiles_emb(drug)\n",
    "        \n",
    "        trans = X.permute(0,2,1)\n",
    "        \n",
    "        drug = self.smiles_Conv1(trans)\n",
    "        drug = self.relu(drug) # 이전엔 추가 안했던 부분\n",
    "        \n",
    "        \n",
    "        drug = self.smiles_Conv2(drug)\n",
    "        drug = self.relu(drug) # 이전엔 추가 안했던 부분\n",
    "        \n",
    "        drug = self.smiles_Conv3(drug)\n",
    "        drug = self.relu(drug) # 이전엔 추가 안했던 부분 \n",
    "#        print(\"drug before maxpool:\",drug.shape)\n",
    "        \n",
    "        drug = self.smiles_AdaMaxPool(drug) # 이전엔 그냥 MaxPool 했던 부분 \n",
    "#        print(\"drug after maxpool:\",drug.shape)        \n",
    "        \n",
    "        drug = drug.view(drug.shape[0],-1)        # 이전엔 추가 안했던 부분 \n",
    "\n",
    "#        drug = torch.max(drug,dim=2).values \n",
    "#        protein = torch.max(protein,dim=2).values # MaxPooling하면 96개의 channel들에 대해서 Maxpooling이 되어서 이 96개 처리 어떻게 할지 https://kaya-dev.tistory.com/6 참고했음\n",
    "  \n",
    "        \n",
    "        X = self.protein_emb(protein) \n",
    "        \n",
    "        trans = X.permute(0,2,1)\n",
    "        \n",
    "        protein = self.protein_Conv1(trans)\n",
    "        protein = self.relu(protein) # 이전엔 추가 안했던 부분        \n",
    "        \n",
    "        protein = self.protein_Conv2(protein)\n",
    "        protein = self.relu(protein) # 이전엔 추가 안했던 부분        \n",
    "        \n",
    "        protein = self.protein_Conv3(protein)        \n",
    "        protein = self.relu(protein) # 이전엔 추가 안했던 부분        \n",
    "\n",
    "        protein = self.protein_AdaMaxPool(protein)  # 이전엔 그냥 MaxPool 했던 부분 \n",
    "        \n",
    "        protein = protein.view(protein.shape[0],-1)       # 이전엔 추가 안했던 부분 \n",
    "        \n",
    "       # print(drug.shape, protein.shape)#\n",
    "                \n",
    "        drug_protein = torch.cat([drug,protein],dim = 1) \n",
    "        \n",
    "        drug_protein = self.linear1(drug_protein)\n",
    "        drug_protein = self.relu(drug_protein)\n",
    "        drug_protein = self.dropout1(drug_protein)\n",
    "        \n",
    "        drug_protein = self.linear2(drug_protein)\n",
    "        drug_protein = self.relu(drug_protein)\n",
    "        drug_protein = self.dropout2(drug_protein)\n",
    "\n",
    "        drug_protein = self.linear3(drug_protein)\n",
    "        drug_protein = self.relu(drug_protein)        \n",
    "        \n",
    "        #drug_protein = self.linear4(drug_protein)    \n",
    "        \n",
    "        drug_protein = self.linear_final(drug_protein)\n",
    "        \n",
    "        return drug_protein\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d4c93",
   "metadata": {},
   "source": [
    "- AdaptiveMaxPool1d 관련: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1312d50f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "_protein_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f6d4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bc2ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9ac7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y,f): #measure : mean squared error\n",
    "    mse = ((y - f)**2).mean(axis=0)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8e672d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[0/100] train_loss: 1.05410 test_loss: 0.44974The Time is:', 5.72038197517395)\n",
      "('[1/100] train_loss: 0.50288 test_loss: 0.41310The Time is:', 5.69394850730896)\n",
      "('[2/100] train_loss: 0.49453 test_loss: 0.53387The Time is:', 5.676478862762451)\n",
      "('[3/100] train_loss: 0.49655 test_loss: 0.43397The Time is:', 5.6628618240356445)\n",
      "('[4/100] train_loss: 0.51443 test_loss: 0.52959The Time is:', 5.721963882446289)\n",
      "('[5/100] train_loss: 0.50477 test_loss: 0.41002The Time is:', 5.677354097366333)\n",
      "('[6/100] train_loss: 0.49810 test_loss: 0.40900The Time is:', 5.733128547668457)\n",
      "('[7/100] train_loss: 0.53337 test_loss: 0.43228The Time is:', 5.75837779045105)\n",
      "('[8/100] train_loss: 0.47489 test_loss: 0.41465The Time is:', 5.8811421394348145)\n",
      "('[9/100] train_loss: 0.46325 test_loss: 0.39126The Time is:', 5.819016933441162)\n",
      "('[10/100] train_loss: 0.45786 test_loss: 0.37501The Time is:', 5.7138283252716064)\n",
      "('[11/100] train_loss: 0.43660 test_loss: 0.36046The Time is:', 5.719956636428833)\n",
      "('[12/100] train_loss: 0.43218 test_loss: 0.35945The Time is:', 5.799544334411621)\n",
      "('[13/100] train_loss: 0.39583 test_loss: 0.41730The Time is:', 5.692755937576294)\n",
      "('[14/100] train_loss: 0.45426 test_loss: 0.45545The Time is:', 5.736624717712402)\n",
      "('[15/100] train_loss: 0.37699 test_loss: 0.39641The Time is:', 5.680254697799683)\n",
      "('[16/100] train_loss: 0.35495 test_loss: 0.30985The Time is:', 5.719963073730469)\n",
      "('[17/100] train_loss: 0.34589 test_loss: 0.41131The Time is:', 5.738556146621704)\n",
      "('[18/100] train_loss: 0.35260 test_loss: 0.32658The Time is:', 5.80390739440918)\n",
      "('[19/100] train_loss: 0.33001 test_loss: 0.30709The Time is:', 5.713284492492676)\n",
      "('[20/100] train_loss: 0.32200 test_loss: 0.31337The Time is:', 5.754335403442383)\n",
      "('[21/100] train_loss: 0.31960 test_loss: 0.39502The Time is:', 5.753126382827759)\n",
      "('[22/100] train_loss: 0.31252 test_loss: 0.30132The Time is:', 5.80916166305542)\n",
      "('[23/100] train_loss: 0.29740 test_loss: 0.29952The Time is:', 5.772137403488159)\n",
      "('[24/100] train_loss: 0.29900 test_loss: 0.29203The Time is:', 5.787988662719727)\n",
      "('[25/100] train_loss: 0.28064 test_loss: 0.28409The Time is:', 5.772910833358765)\n",
      "('[26/100] train_loss: 0.30301 test_loss: 0.27464The Time is:', 5.773119688034058)\n",
      "('[27/100] train_loss: 0.26250 test_loss: 0.26600The Time is:', 5.78672194480896)\n",
      "('[28/100] train_loss: 0.26546 test_loss: 0.26203The Time is:', 5.791531085968018)\n",
      "('[29/100] train_loss: 0.24420 test_loss: 0.30785The Time is:', 5.802673578262329)\n",
      "('[30/100] train_loss: 0.23807 test_loss: 0.25221The Time is:', 5.741150379180908)\n",
      "('[31/100] train_loss: 0.22147 test_loss: 0.24555The Time is:', 5.8257691860198975)\n",
      "('[32/100] train_loss: 0.21546 test_loss: 0.25466The Time is:', 5.8402464389801025)\n",
      "('[33/100] train_loss: 0.24597 test_loss: 0.24437The Time is:', 5.772626161575317)\n",
      "('[34/100] train_loss: 0.20396 test_loss: 0.25170The Time is:', 5.786797761917114)\n",
      "('[35/100] train_loss: 0.19662 test_loss: 0.25035The Time is:', 5.798085927963257)\n",
      "('[36/100] train_loss: 0.18723 test_loss: 0.26508The Time is:', 5.834707021713257)\n",
      "('[37/100] train_loss: 0.17735 test_loss: 0.23523The Time is:', 5.818769693374634)\n",
      "('[38/100] train_loss: 0.16634 test_loss: 0.23126The Time is:', 5.813333749771118)\n",
      "('[39/100] train_loss: 0.16257 test_loss: 0.24397The Time is:', 5.822452545166016)\n",
      "('[40/100] train_loss: 0.15221 test_loss: 0.23260The Time is:', 5.810901880264282)\n",
      "('[41/100] train_loss: 0.15385 test_loss: 0.25696The Time is:', 5.8105151653289795)\n",
      "('[42/100] train_loss: 0.14526 test_loss: 0.23174The Time is:', 5.865030765533447)\n",
      "('[43/100] train_loss: 0.15140 test_loss: 0.22104The Time is:', 5.785454750061035)\n",
      "('[44/100] train_loss: 0.14396 test_loss: 0.24702The Time is:', 5.8199779987335205)\n",
      "('[45/100] train_loss: 0.13201 test_loss: 0.22233The Time is:', 5.798005104064941)\n",
      "('[46/100] train_loss: 0.12605 test_loss: 0.22077The Time is:', 5.77387809753418)\n",
      "('[47/100] train_loss: 0.12318 test_loss: 0.21819The Time is:', 5.814245939254761)\n",
      "('[48/100] train_loss: 0.11624 test_loss: 0.21681The Time is:', 5.9423158168792725)\n",
      "('[49/100] train_loss: 0.11484 test_loss: 0.22894The Time is:', 5.862314701080322)\n",
      "('[50/100] train_loss: 0.11359 test_loss: 0.21766The Time is:', 5.895094633102417)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4882/1856368525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chihyeon/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chihyeon/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        drugs,targets,labels = data[:,:85], data[:,85:1200+85], data[:,-1]\n",
    "        drugs = drugs.int()\n",
    "        targets = targets.int()\n",
    "        drugs = drugs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(drugs,targets)\n",
    "\n",
    "        #print(\"Training:  output[0]: \", output[0].item(), \"  labels[0]\",labels[0].item())        \n",
    "\n",
    "        loss = loss_fn(output, labels.view(-1,1).float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.average(train_losses)\n",
    "\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "\n",
    "            drugs,targets,labels = data[:,:85], data[:,85:1200+85], data[:,-1]\n",
    "            drugs = drugs.int()\n",
    "            targets = targets.int()\n",
    "            drugs = drugs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            labels = labels.to(device)                \n",
    "            output = model(drugs,targets)\n",
    "            \n",
    "#            print(\"Validation:  output[0]: \", output[0].item(), \"  labels[0]\",labels[0].item())\n",
    "            \n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, labels.view(-1,1).cpu()), 0)\n",
    "    G,P = total_labels.numpy().flatten(),total_preds.numpy().flatten()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # epoch당 평균 loss 계산\n",
    "    train_loss = np.average(train_losses)\n",
    "    test_loss = mse(G,P)\n",
    "\n",
    "    epoch_len = len(str(NUM_EPOCHS))\n",
    "\n",
    "\n",
    "    print_msg = (f'[{epoch}/{NUM_EPOCHS}] ' +\n",
    "                 f'train_loss: {train_loss:.5f} ' +\n",
    "                 f'test_loss: {test_loss:.5f}' +\n",
    "                \"The Time is:\",time.time() - start)\n",
    "\n",
    "    print(print_msg)\n",
    "\n",
    "    # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "\n",
    "    counter = 0 #patience\n",
    "    best_mse = test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f689be5-d72e-403c-a3bd-47e743586699",
   "metadata": {},
   "source": [
    "- 내가 만든 training 코드에서는 loss값의 단위가 논문과 맞기 않게 나와서 다른 코드를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7ee18-1f89-499c-9762-a93d4122dc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f25a4bb-6fbe-4c4c-ab40-62e795215ee6",
   "metadata": {},
   "source": [
    "`-` 다른 분이 구현한 DeepDTA처럼 validation loss가 떨어지긴 했는데, 그 분은 100 epoch해서 loss 0.25가 나온 반면, 내 것은 34 epoch 정도에서 나왔었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28802a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d805f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f4a948-7901-4d02-8c17-c272305abc53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 모델 만드는 중에 마주친 문제 및 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423066a-dca5-49a2-b88c-5d9b3d578d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dbe7138-01ad-47f8-9d6f-ad911bb1dc25",
   "metadata": {},
   "source": [
    "- CUDA error: device-side assert triggered\n",
    "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1. \n",
    "    - 에러 해결책 ->  https://mopipe.tistory.com/180\n",
    "    \n",
    "- CUDA관련 여러 에러 이슈들: https://nuggy875.tistory.com/135"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1648738-b5b8-4e96-bf75-4f4c912446aa",
   "metadata": {},
   "source": [
    "1. 어떻게 word embeding으로 해서 각 단어에 대해 1dconv를 적용할지 -> 해결\n",
    "    - https://happy-jihye.github.io/nlp/nlp-4/\n",
    "    - https://wikidocs.net/80437\n",
    "    - https://iamseungjun.tistory.com/15\n",
    "    - https://kaya-dev.tistory.com/6 -> word embedding + MaxPooling\n",
    "    \n",
    "    <br/>\n",
    "2. 256개의 데이터만 계속 보면 이 training set에 대해선 적어도 loss가 줄어야되는데 전혀 줄지가 않았다.\n",
    "    - 원인은 모델 파라메터를 optimizer에 넘겨주는 코드를 실행하고 그 다음에 다시 모델 인스턴스를 만드는 순서로 갔기 때문이다. 반드시 모델을 선언한 다음에 optimizer에 모델 parameter 넘겨줘야한다.     \n",
    "    \n",
    "    \n",
    "    <br/>    \n",
    "3.  loss의 scale이 100 단위로 나오는 것이 잘 작동하는 것 같지 않다.\n",
    "    - https://stackoverflow.com/questions/59153248/why-is-my-neural-network-stuck-at-high-loss-value-after-the-first-epochs\n",
    "    - https://www.google.com/search?q=the+scale+of+loss+is+too+huge&oq=the+scale+of+loss+is+too+huge&aqs=chrome..69i57j33i160.4544j0j4&sourceid=chrome&ie=UTF-8\n",
    "\n",
    "    - 일단 label shape가 data[:,-1]로 해야되는데 data[-1]로 했었다. 일단 이것을 data[:,-1] 로 바꿔서 1.0979899 까지 loss 내려간 것을 확인했다.\n",
    "    \n",
    "    - 다음은 output shape가 output.shape torch.Size([256, 96, 1])  인게 이상함. 이것도 256,1 꼴로 나와야될텐데, filter 처리를 제대로 못한 것 같음\n",
    "    \n",
    "    <br/>\n",
    "4. embedding 벡터 shape 변환시키는 과정 시간이 너무 오래걸린다. -> 여러가지 고치고 나니까 해결되었다.\n",
    "\n",
    "\n",
    "    <br/>\n",
    "5. cuda를 데이터랑 모델에 했는데도 cuda, cpu안맞는다고 에러가 나왔는데 그 이유가, 내가 model class의 forward에서 embedding shape 바꾸려고 trans 텐서를 따로 선언한 것에 대해서 to cuda를 안한 것이 원인이였다.\n",
    "\n",
    "\n",
    "    <br/>\n",
    "6. 모델부터 training코드까지 잘 작성한 상태였고, training loss는 떨어지는 상황이였는데, validation loss만 떨어지지 않았었다.\n",
    "    - 원인은 broadcasting으로 Drug와 Protein 데이터를 병합하고 [:0.5], [0.5:] 와 같은 방식으로 데이터를 나눠서, 모델이 사실상 앞의 0.5비율 만큼만 학습하고 뒤의 Drug는 전혀 학습을 못한 것이 원인이였던 것 같다. 이것을 shuffle하고 training을 다시 하니까 validation loss도 잘 감소하였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
